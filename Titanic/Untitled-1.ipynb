{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\14488\\AppData\\Local\\Temp\\ipykernel_15692\\449023232.py:72: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_train[col].fillna(most_frequent, inplace=True)\n",
      "C:\\Users\\14488\\AppData\\Local\\Temp\\ipykernel_15692\\449023232.py:73: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_test[col].fillna(most_frequent, inplace=True)\n",
      "C:\\Users\\14488\\AppData\\Local\\Temp\\ipykernel_15692\\449023232.py:72: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_train[col].fillna(most_frequent, inplace=True)\n",
      "C:\\Users\\14488\\AppData\\Local\\Temp\\ipykernel_15692\\449023232.py:73: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_test[col].fillna(most_frequent, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6788850\ttotal: 3.15ms\tremaining: 3.15s\n",
      "100:\tlearn: 0.3417354\ttotal: 243ms\tremaining: 2.17s\n",
      "200:\tlearn: 0.2870961\ttotal: 498ms\tremaining: 1.98s\n",
      "300:\tlearn: 0.2550872\ttotal: 752ms\tremaining: 1.75s\n",
      "400:\tlearn: 0.2211030\ttotal: 1.01s\tremaining: 1.51s\n",
      "500:\tlearn: 0.1969145\ttotal: 1.28s\tremaining: 1.28s\n",
      "600:\tlearn: 0.1742177\ttotal: 1.56s\tremaining: 1.04s\n",
      "700:\tlearn: 0.1556685\ttotal: 1.83s\tremaining: 780ms\n",
      "800:\tlearn: 0.1401056\ttotal: 2.1s\tremaining: 521ms\n",
      "900:\tlearn: 0.1276031\ttotal: 2.36s\tremaining: 260ms\n",
      "999:\tlearn: 0.1170511\ttotal: 2.62s\tremaining: 0us\n",
      "ðŸ”¹ Ð¢Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ Ð½Ð° Ð¾Ð±ÑƒÑ‡Ð°ÑŽÑ‰Ð¸Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…: 0.97\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ðŸ”¹ 1. Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# ðŸ”¹ 2. Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Ð½Ð¾Ð²Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸ (Feature Engineering)\n",
    "# 2.1 Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ñ‚Ð¸Ñ‚ÑƒÐ» Ð¸Ð· Ð¸Ð¼ÐµÐ½Ð¸ (Mr, Miss, Mrs Ð¸ Ñ‚. Ð´.)\n",
    "train_df['Title'] = train_df['Name'].apply(lambda x: x.split(', ')[1].split('.')[0])\n",
    "test_df['Title'] = test_df['Name'].apply(lambda x: x.split(', ')[1].split('.')[0])\n",
    "\n",
    "# Ð“Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€ÑƒÐµÐ¼ Ñ€ÐµÐ´ÐºÐ¸Ðµ Ñ‚Ð¸Ñ‚ÑƒÐ»Ñ‹\n",
    "rare_titles = ['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona']\n",
    "train_df['Title'] = train_df['Title'].replace(rare_titles, 'Rare')\n",
    "test_df['Title'] = test_df['Title'].replace(rare_titles, 'Rare')\n",
    "\n",
    "# 2.2 Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Ð¿Ñ€Ð¸Ð·Ð½Ð°Ðº \"Deck\" (Ð¿Ð°Ð»ÑƒÐ±Ð°) Ð¸Ð· Ð½Ð¾Ð¼ÐµÑ€Ð° ÐºÐ°ÑŽÑ‚Ñ‹\n",
    "train_df['Deck'] = train_df['Cabin'].fillna('M').apply(lambda x: x[0])  # M = No Cabin\n",
    "test_df['Deck'] = test_df['Cabin'].fillna('M').apply(lambda x: x[0])\n",
    "\n",
    "# 2.3 Ð¤Ð»Ð°Ð³ \"Ð‘Ñ‹Ð»Ð° Ð»Ð¸ ÐºÐ°ÑŽÑ‚Ð°\"\n",
    "train_df['HasCabin'] = (train_df['Cabin'].notna()).astype(int)\n",
    "test_df['HasCabin'] = (test_df['Cabin'].notna()).astype(int)\n",
    "\n",
    "# 2.4 Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ ÑÐµÐ¼ÐµÐ¹Ð½Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸\n",
    "train_df['FamilySize'] = train_df['SibSp'] + train_df['Parch'] + 1\n",
    "test_df['FamilySize'] = test_df['SibSp'] + test_df['Parch'] + 1\n",
    "\n",
    "train_df['IsAlone'] = (train_df['FamilySize'] == 1).astype(int)\n",
    "test_df['IsAlone'] = (test_df['FamilySize'] == 1).astype(int)\n",
    "\n",
    "# 2.5 Ð”Ñ€ÑƒÐ³Ð¸Ðµ Ð¿Ð¾Ð»ÐµÐ·Ð½Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸\n",
    "train_df['Fare_Per_Person'] = train_df['Fare'] / train_df['FamilySize']\n",
    "test_df['Fare_Per_Person'] = test_df['Fare'] / test_df['FamilySize']\n",
    "\n",
    "train_df['Age*Class'] = train_df['Age'] * train_df['Pclass']\n",
    "test_df['Age*Class'] = test_df['Age'] * test_df['Pclass']\n",
    "\n",
    "# ðŸ”¹ 3. Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ PassengerId Ð´Ð»Ñ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ ÑÐ°Ð±Ð¼Ð¸Ñ‚Ð°\n",
    "passenger_ids = test_df['PassengerId']\n",
    "\n",
    "# ðŸ”¹ 4. Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð½ÐµÐ½ÑƒÐ¶Ð½Ñ‹Ðµ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸\n",
    "drop_cols = ['PassengerId', 'Name', 'Ticket', 'Cabin']\n",
    "train_df = train_df.drop(drop_cols, axis=1)\n",
    "test_df = test_df.drop(drop_cols, axis=1)\n",
    "\n",
    "# ðŸ”¹ 5. Ð Ð°Ð·Ð´ÐµÐ»ÑÐµÐ¼ Ð½Ð° Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸ (X) Ð¸ Ñ†ÐµÐ»ÐµÐ²ÑƒÑŽ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½ÑƒÑŽ (y)\n",
    "X_train = train_df.drop('Survived', axis=1)\n",
    "y_train = train_df['Survived']\n",
    "X_test = test_df\n",
    "\n",
    "# ðŸ”¹ 6. Ð—Ð°Ð¿Ð¾Ð»Ð½ÑÐµÐ¼ Ð¿Ñ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½Ð½Ñ‹Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ (Age, Fare, Embarked)\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train[['Age', 'Fare', 'Age*Class']] = imputer.fit_transform(X_train[['Age', 'Fare', 'Age*Class']])\n",
    "X_test[['Age', 'Fare', 'Age*Class']] = imputer.transform(X_test[['Age', 'Fare', 'Age*Class']])\n",
    "\n",
    "# ÐŸÐµÑ€ÐµÑÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ Fare_Per_Person Ð¿Ð¾ÑÐ»Ðµ Ð·Ð°Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ Fare\n",
    "X_train['Fare_Per_Person'] = X_train['Fare'] / X_train['FamilySize']\n",
    "X_test['Fare_Per_Person'] = X_test['Fare'] / X_test['FamilySize']\n",
    "\n",
    "# ðŸ”¹ 7. Ð—Ð°Ð¿Ð¾Ð»Ð½ÑÐµÐ¼ Ð¿Ñ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½Ð½Ñ‹Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ Ð² ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ°Ñ…\n",
    "categorical_features = ['Sex', 'Embarked', 'Pclass', 'Title', 'Deck']\n",
    "for col in categorical_features:\n",
    "    most_frequent = X_train[col].mode()[0]\n",
    "    X_train[col].fillna(most_frequent, inplace=True)\n",
    "    X_test[col].fillna(most_frequent, inplace=True)\n",
    "\n",
    "# ðŸ”¹ 8. ÐšÐ¾Ð´Ð¸Ñ€ÑƒÐµÐ¼ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð¸ Ð´Ð»Ñ **RandomForest** Ð¸ **GradientBoosting**\n",
    "le = LabelEncoder()\n",
    "for col in categorical_features:\n",
    "    X_train[col + '_num'] = le.fit_transform(X_train[col])\n",
    "    X_test[col + '_num'] = le.transform(X_test[col])\n",
    "\n",
    "# Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð¸ÑÑ…Ð¾Ð´Ð½Ñ‹Ðµ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ðµ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸\n",
    "X_train = X_train.drop(categorical_features, axis=1)\n",
    "X_test = X_test.drop(categorical_features, axis=1)\n",
    "\n",
    "# ðŸ”¹ 9. ÐžÐ±ÑƒÑ‡Ð°ÐµÐ¼ CatBoost\n",
    "# Ð”Ð»Ñ CatBoost Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¸ÑÑ…Ð¾Ð´Ð½Ñ‹Ðµ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸\n",
    "X_train_catboost = X_train.copy()\n",
    "X_test_catboost = X_test.copy()\n",
    "\n",
    "catboost_model = CatBoostClassifier(iterations=1000, learning_rate=0.02, depth=8, l2_leaf_reg=3, random_seed=42, verbose=100)\n",
    "catboost_model.fit(X_train_catboost, y_train)\n",
    "\n",
    "# ðŸ”¹ 10. ÐžÐ±ÑƒÑ‡Ð°ÐµÐ¼ RandomForest Ð¸ GradientBoosting\n",
    "rf_model = RandomForestClassifier(n_estimators=300, max_depth=10, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "gb_model = GradientBoostingClassifier(n_estimators=300, learning_rate=0.05, max_depth=5, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# ðŸ”¹ 11. Ð£ÑÑ€ÐµÐ´Ð½ÑÐµÐ¼ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ (CatBoost + RF + GB)\n",
    "cat_preds = catboost_model.predict_proba(X_test_catboost)[:, 1]\n",
    "rf_preds = rf_model.predict_proba(X_test)[:, 1]\n",
    "gb_preds = gb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "final_predictions = (cat_preds * 0.5 + rf_preds * 0.3 + gb_preds * 0.2) > 0.5\n",
    "final_predictions = final_predictions.astype(int)\n",
    "\n",
    "# ðŸ”¹ 12. Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Ñ„Ð°Ð¹Ð» gender_submission.csv\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': passenger_ids,\n",
    "    'Survived': final_predictions\n",
    "})\n",
    "\n",
    "# Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ Ð² Ñ„Ð°Ð¹Ð»\n",
    "submission.to_csv('gender_submission.csv', index=False)\n",
    "\n",
    "# ðŸ”¹ 13. ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ Ð½Ð° Ð¾Ð±ÑƒÑ‡Ð°ÑŽÑ‰Ð¸Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… (Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸)\n",
    "train_predictions = catboost_model.predict(X_train_catboost)\n",
    "accuracy = accuracy_score(y_train, train_predictions)\n",
    "print(f\"ðŸ”¹ Ð¢Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ Ð½Ð° Ð¾Ð±ÑƒÑ‡Ð°ÑŽÑ‰Ð¸Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "catboost CV Score: 0.8294 (+/- 0.0812)\n",
      "random_forest CV Score: 0.8350 (+/- 0.0706)\n",
      "gradient_boosting CV Score: 0.8317 (+/- 0.0585)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Missing values found in test features",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 183\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;28mprint\u001b[39m(classification_report(y_train, train_preds))\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 183\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 157\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m X_test \u001b[38;5;241m=\u001b[39m processed_test[feature_cols]\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# Verify no missing values in test data\u001b[39;00m\n\u001b[1;32m--> 157\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m X_test\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39many()\u001b[38;5;241m.\u001b[39many(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing values found in test features\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m    160\u001b[0m weights \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcatboost\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_forest\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgradient_boosting\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.2\u001b[39m}\n",
      "\u001b[1;31mAssertionError\u001b[0m: Missing values found in test features"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "class TitanicPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.categorical_features = ['Sex', 'Embarked', 'Pclass', 'Title', 'Deck']\n",
    "        self.numerical_features = ['Age', 'Fare', 'SibSp', 'Parch']\n",
    "        self.label_encoders = {}\n",
    "        self.numerical_imputer = SimpleImputer(strategy='median')\n",
    "        self.categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    \n",
    "    def extract_title(self, name):\n",
    "        title = name.split(', ')[1].split('.')[0]\n",
    "        rare_titles = ['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', \n",
    "                      'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona']\n",
    "        return 'Rare' if title in rare_titles else title\n",
    "    \n",
    "    def create_features(self, df):\n",
    "        \"\"\"Create all feature engineering in one place\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Extract title from name\n",
    "        df['Title'] = df['Name'].apply(self.extract_title)\n",
    "        \n",
    "        # Cabin features\n",
    "        df['Deck'] = df['Cabin'].fillna('M').str[0]\n",
    "        df['HasCabin'] = df['Cabin'].notna().astype(int)\n",
    "        \n",
    "        # Family features\n",
    "        df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "        df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n",
    "        \n",
    "        # Fare features\n",
    "        df['Fare_Per_Person'] = df['Fare'] / df['FamilySize']\n",
    "        \n",
    "        # Age interaction will be computed after imputation\n",
    "        return df\n",
    "    \n",
    "    def fit_transform(self, df):\n",
    "        \"\"\"Full preprocessing pipeline for training data\"\"\"\n",
    "        # Create initial features\n",
    "        df = self.create_features(df)\n",
    "        \n",
    "        # Handle missing values in numerical features\n",
    "        numerical_features_matrix = self.numerical_imputer.fit_transform(df[self.numerical_features])\n",
    "        df[self.numerical_features] = numerical_features_matrix\n",
    "        \n",
    "        # Now we can safely compute Age*Class\n",
    "        df['Age*Class'] = df['Age'] * df['Pclass']\n",
    "        \n",
    "        # Handle missing values in categorical features\n",
    "        categorical_features_matrix = self.categorical_imputer.fit_transform(df[self.categorical_features])\n",
    "        df[self.categorical_features] = categorical_features_matrix\n",
    "        \n",
    "        # Encode categorical features\n",
    "        for col in self.categorical_features:\n",
    "            self.label_encoders[col] = LabelEncoder()\n",
    "            df[col + '_encoded'] = self.label_encoders[col].fit_transform(df[col])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \"\"\"Transform test data using fitted preprocessor\"\"\"\n",
    "        # Create initial features\n",
    "        df = self.create_features(df)\n",
    "        \n",
    "        # Handle missing values using fitted imputers\n",
    "        numerical_features_matrix = self.numerical_imputer.transform(df[self.numerical_features])\n",
    "        df[self.numerical_features] = numerical_features_matrix\n",
    "        \n",
    "        # Now we can safely compute Age*Class\n",
    "        df['Age*Class'] = df['Age'] * df['Pclass']\n",
    "        \n",
    "        categorical_features_matrix = self.categorical_imputer.transform(df[self.categorical_features])\n",
    "        df[self.categorical_features] = categorical_features_matrix\n",
    "        \n",
    "        # Use fitted label encoders\n",
    "        for col in self.categorical_features:\n",
    "            df[col + '_encoded'] = self.label_encoders[col].transform(df[col])\n",
    "        \n",
    "        return df\n",
    "\n",
    "def train_models(X_train, y_train):\n",
    "    \"\"\"Train multiple models with cross-validation\"\"\"\n",
    "    models = {\n",
    "        'catboost': CatBoostClassifier(\n",
    "            iterations=1000,\n",
    "            learning_rate=0.02,\n",
    "            depth=8,\n",
    "            l2_leaf_reg=3,\n",
    "            random_seed=42,\n",
    "            verbose=False\n",
    "        ),\n",
    "        'random_forest': RandomForestClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=10,\n",
    "            random_state=42\n",
    "        ),\n",
    "        'gradient_boosting': GradientBoostingClassifier(\n",
    "            n_estimators=300,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=5,\n",
    "            random_state=42\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    trained_models = {}\n",
    "    for name, model in models.items():\n",
    "        # Perform cross-validation\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "        print(f\"{name} CV Score: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n",
    "        \n",
    "        # Train on full training data\n",
    "        model.fit(X_train, y_train)\n",
    "        trained_models[name] = model\n",
    "    \n",
    "    return trained_models\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    train_df = pd.read_csv('train.csv')\n",
    "    test_df = pd.read_csv('test.csv')\n",
    "    \n",
    "    # Initialize preprocessor\n",
    "    preprocessor = TitanicPreprocessor()\n",
    "    \n",
    "    # Preprocess training data\n",
    "    processed_train = preprocessor.fit_transform(train_df)\n",
    "    \n",
    "    # Prepare features for training\n",
    "    feature_cols = ([f\"{col}_encoded\" for col in preprocessor.categorical_features] + \n",
    "                   preprocessor.numerical_features + \n",
    "                   ['HasCabin', 'IsAlone', 'Fare_Per_Person', 'Age*Class'])\n",
    "    \n",
    "    X_train = processed_train[feature_cols]\n",
    "    y_train = processed_train['Survived']\n",
    "    \n",
    "    # Verify no missing values\n",
    "    assert not X_train.isnull().any().any(), \"Missing values found in training features\"\n",
    "    \n",
    "    # Train models\n",
    "    trained_models = train_models(X_train, y_train)\n",
    "    \n",
    "    # Process test data\n",
    "    processed_test = preprocessor.transform(test_df)\n",
    "    X_test = processed_test[feature_cols]\n",
    "    \n",
    "    # Verify no missing values in test data\n",
    "    assert not X_test.isnull().any().any(), \"Missing values found in test features\"\n",
    "    \n",
    "    # Make predictions\n",
    "    weights = {'catboost': 0.5, 'random_forest': 0.3, 'gradient_boosting': 0.2}\n",
    "    weighted_predictions = np.zeros(len(X_test))\n",
    "    \n",
    "    for name, model in trained_models.items():\n",
    "        weighted_predictions += model.predict_proba(X_test)[:, 1] * weights[name]\n",
    "    \n",
    "    final_predictions = (weighted_predictions > 0.5).astype(int)\n",
    "    \n",
    "    # Create submission file\n",
    "    submission = pd.DataFrame({\n",
    "        'PassengerId': test_df['PassengerId'],\n",
    "        'Survived': final_predictions\n",
    "    })\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    \n",
    "    # Print training metrics\n",
    "    print(\"\\nTraining Metrics:\")\n",
    "    for name, model in trained_models.items():\n",
    "        train_preds = model.predict(X_train)\n",
    "        print(f\"\\n{name} Performance:\")\n",
    "        print(classification_report(y_train, train_preds))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
