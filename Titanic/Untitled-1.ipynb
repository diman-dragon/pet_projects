{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\14488\\AppData\\Local\\Temp\\ipykernel_15692\\449023232.py:72: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_train[col].fillna(most_frequent, inplace=True)\n",
      "C:\\Users\\14488\\AppData\\Local\\Temp\\ipykernel_15692\\449023232.py:73: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_test[col].fillna(most_frequent, inplace=True)\n",
      "C:\\Users\\14488\\AppData\\Local\\Temp\\ipykernel_15692\\449023232.py:72: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_train[col].fillna(most_frequent, inplace=True)\n",
      "C:\\Users\\14488\\AppData\\Local\\Temp\\ipykernel_15692\\449023232.py:73: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_test[col].fillna(most_frequent, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6788850\ttotal: 3.15ms\tremaining: 3.15s\n",
      "100:\tlearn: 0.3417354\ttotal: 243ms\tremaining: 2.17s\n",
      "200:\tlearn: 0.2870961\ttotal: 498ms\tremaining: 1.98s\n",
      "300:\tlearn: 0.2550872\ttotal: 752ms\tremaining: 1.75s\n",
      "400:\tlearn: 0.2211030\ttotal: 1.01s\tremaining: 1.51s\n",
      "500:\tlearn: 0.1969145\ttotal: 1.28s\tremaining: 1.28s\n",
      "600:\tlearn: 0.1742177\ttotal: 1.56s\tremaining: 1.04s\n",
      "700:\tlearn: 0.1556685\ttotal: 1.83s\tremaining: 780ms\n",
      "800:\tlearn: 0.1401056\ttotal: 2.1s\tremaining: 521ms\n",
      "900:\tlearn: 0.1276031\ttotal: 2.36s\tremaining: 260ms\n",
      "999:\tlearn: 0.1170511\ttotal: 2.62s\tremaining: 0us\n",
      "🔹 Точность на обучающих данных: 0.97\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 🔹 1. Загружаем данные\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# 🔹 2. Создаём новые признаки (Feature Engineering)\n",
    "# 2.1 Извлекаем титул из имени (Mr, Miss, Mrs и т. д.)\n",
    "train_df['Title'] = train_df['Name'].apply(lambda x: x.split(', ')[1].split('.')[0])\n",
    "test_df['Title'] = test_df['Name'].apply(lambda x: x.split(', ')[1].split('.')[0])\n",
    "\n",
    "# Группируем редкие титулы\n",
    "rare_titles = ['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona']\n",
    "train_df['Title'] = train_df['Title'].replace(rare_titles, 'Rare')\n",
    "test_df['Title'] = test_df['Title'].replace(rare_titles, 'Rare')\n",
    "\n",
    "# 2.2 Создаём признак \"Deck\" (палуба) из номера каюты\n",
    "train_df['Deck'] = train_df['Cabin'].fillna('M').apply(lambda x: x[0])  # M = No Cabin\n",
    "test_df['Deck'] = test_df['Cabin'].fillna('M').apply(lambda x: x[0])\n",
    "\n",
    "# 2.3 Флаг \"Была ли каюта\"\n",
    "train_df['HasCabin'] = (train_df['Cabin'].notna()).astype(int)\n",
    "test_df['HasCabin'] = (test_df['Cabin'].notna()).astype(int)\n",
    "\n",
    "# 2.4 Создаём семейные признаки\n",
    "train_df['FamilySize'] = train_df['SibSp'] + train_df['Parch'] + 1\n",
    "test_df['FamilySize'] = test_df['SibSp'] + test_df['Parch'] + 1\n",
    "\n",
    "train_df['IsAlone'] = (train_df['FamilySize'] == 1).astype(int)\n",
    "test_df['IsAlone'] = (test_df['FamilySize'] == 1).astype(int)\n",
    "\n",
    "# 2.5 Другие полезные признаки\n",
    "train_df['Fare_Per_Person'] = train_df['Fare'] / train_df['FamilySize']\n",
    "test_df['Fare_Per_Person'] = test_df['Fare'] / test_df['FamilySize']\n",
    "\n",
    "train_df['Age*Class'] = train_df['Age'] * train_df['Pclass']\n",
    "test_df['Age*Class'] = test_df['Age'] * test_df['Pclass']\n",
    "\n",
    "# 🔹 3. Сохраняем PassengerId для финального сабмита\n",
    "passenger_ids = test_df['PassengerId']\n",
    "\n",
    "# 🔹 4. Убираем ненужные колонки\n",
    "drop_cols = ['PassengerId', 'Name', 'Ticket', 'Cabin']\n",
    "train_df = train_df.drop(drop_cols, axis=1)\n",
    "test_df = test_df.drop(drop_cols, axis=1)\n",
    "\n",
    "# 🔹 5. Разделяем на признаки (X) и целевую переменную (y)\n",
    "X_train = train_df.drop('Survived', axis=1)\n",
    "y_train = train_df['Survived']\n",
    "X_test = test_df\n",
    "\n",
    "# 🔹 6. Заполняем пропущенные значения (Age, Fare, Embarked)\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train[['Age', 'Fare', 'Age*Class']] = imputer.fit_transform(X_train[['Age', 'Fare', 'Age*Class']])\n",
    "X_test[['Age', 'Fare', 'Age*Class']] = imputer.transform(X_test[['Age', 'Fare', 'Age*Class']])\n",
    "\n",
    "# Пересчитываем Fare_Per_Person после заполнения Fare\n",
    "X_train['Fare_Per_Person'] = X_train['Fare'] / X_train['FamilySize']\n",
    "X_test['Fare_Per_Person'] = X_test['Fare'] / X_test['FamilySize']\n",
    "\n",
    "# 🔹 7. Заполняем пропущенные значения в категориальных признаках\n",
    "categorical_features = ['Sex', 'Embarked', 'Pclass', 'Title', 'Deck']\n",
    "for col in categorical_features:\n",
    "    most_frequent = X_train[col].mode()[0]\n",
    "    X_train[col].fillna(most_frequent, inplace=True)\n",
    "    X_test[col].fillna(most_frequent, inplace=True)\n",
    "\n",
    "# 🔹 8. Кодируем категории для **RandomForest** и **GradientBoosting**\n",
    "le = LabelEncoder()\n",
    "for col in categorical_features:\n",
    "    X_train[col + '_num'] = le.fit_transform(X_train[col])\n",
    "    X_test[col + '_num'] = le.transform(X_test[col])\n",
    "\n",
    "# Удаляем исходные категориальные колонки\n",
    "X_train = X_train.drop(categorical_features, axis=1)\n",
    "X_test = X_test.drop(categorical_features, axis=1)\n",
    "\n",
    "# 🔹 9. Обучаем CatBoost\n",
    "# Для CatBoost используем исходные категориальные признаки\n",
    "X_train_catboost = X_train.copy()\n",
    "X_test_catboost = X_test.copy()\n",
    "\n",
    "catboost_model = CatBoostClassifier(iterations=1000, learning_rate=0.02, depth=8, l2_leaf_reg=3, random_seed=42, verbose=100)\n",
    "catboost_model.fit(X_train_catboost, y_train)\n",
    "\n",
    "# 🔹 10. Обучаем RandomForest и GradientBoosting\n",
    "rf_model = RandomForestClassifier(n_estimators=300, max_depth=10, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "gb_model = GradientBoostingClassifier(n_estimators=300, learning_rate=0.05, max_depth=5, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# 🔹 11. Усредняем предсказания (CatBoost + RF + GB)\n",
    "cat_preds = catboost_model.predict_proba(X_test_catboost)[:, 1]\n",
    "rf_preds = rf_model.predict_proba(X_test)[:, 1]\n",
    "gb_preds = gb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "final_predictions = (cat_preds * 0.5 + rf_preds * 0.3 + gb_preds * 0.2) > 0.5\n",
    "final_predictions = final_predictions.astype(int)\n",
    "\n",
    "# 🔹 12. Создаём файл gender_submission.csv\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': passenger_ids,\n",
    "    'Survived': final_predictions\n",
    "})\n",
    "\n",
    "# Сохраняем предсказания в файл\n",
    "submission.to_csv('gender_submission.csv', index=False)\n",
    "\n",
    "# 🔹 13. Проверяем точность на обучающих данных (для оценки)\n",
    "train_predictions = catboost_model.predict(X_train_catboost)\n",
    "accuracy = accuracy_score(y_train, train_predictions)\n",
    "print(f\"🔹 Точность на обучающих данных: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "catboost CV Score: 0.8294 (+/- 0.0812)\n",
      "random_forest CV Score: 0.8350 (+/- 0.0706)\n",
      "gradient_boosting CV Score: 0.8317 (+/- 0.0585)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Missing values found in test features",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 183\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;28mprint\u001b[39m(classification_report(y_train, train_preds))\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 183\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 157\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m X_test \u001b[38;5;241m=\u001b[39m processed_test[feature_cols]\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# Verify no missing values in test data\u001b[39;00m\n\u001b[1;32m--> 157\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m X_test\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39many()\u001b[38;5;241m.\u001b[39many(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing values found in test features\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m    160\u001b[0m weights \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcatboost\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_forest\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgradient_boosting\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.2\u001b[39m}\n",
      "\u001b[1;31mAssertionError\u001b[0m: Missing values found in test features"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "class TitanicPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.categorical_features = ['Sex', 'Embarked', 'Pclass', 'Title', 'Deck']\n",
    "        self.numerical_features = ['Age', 'Fare', 'SibSp', 'Parch']\n",
    "        self.label_encoders = {}\n",
    "        self.numerical_imputer = SimpleImputer(strategy='median')\n",
    "        self.categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    \n",
    "    def extract_title(self, name):\n",
    "        title = name.split(', ')[1].split('.')[0]\n",
    "        rare_titles = ['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', \n",
    "                      'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona']\n",
    "        return 'Rare' if title in rare_titles else title\n",
    "    \n",
    "    def create_features(self, df):\n",
    "        \"\"\"Create all feature engineering in one place\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Extract title from name\n",
    "        df['Title'] = df['Name'].apply(self.extract_title)\n",
    "        \n",
    "        # Cabin features\n",
    "        df['Deck'] = df['Cabin'].fillna('M').str[0]\n",
    "        df['HasCabin'] = df['Cabin'].notna().astype(int)\n",
    "        \n",
    "        # Family features\n",
    "        df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "        df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n",
    "        \n",
    "        # Fare features\n",
    "        df['Fare_Per_Person'] = df['Fare'] / df['FamilySize']\n",
    "        \n",
    "        # Age interaction will be computed after imputation\n",
    "        return df\n",
    "    \n",
    "    def fit_transform(self, df):\n",
    "        \"\"\"Full preprocessing pipeline for training data\"\"\"\n",
    "        # Create initial features\n",
    "        df = self.create_features(df)\n",
    "        \n",
    "        # Handle missing values in numerical features\n",
    "        numerical_features_matrix = self.numerical_imputer.fit_transform(df[self.numerical_features])\n",
    "        df[self.numerical_features] = numerical_features_matrix\n",
    "        \n",
    "        # Now we can safely compute Age*Class\n",
    "        df['Age*Class'] = df['Age'] * df['Pclass']\n",
    "        \n",
    "        # Handle missing values in categorical features\n",
    "        categorical_features_matrix = self.categorical_imputer.fit_transform(df[self.categorical_features])\n",
    "        df[self.categorical_features] = categorical_features_matrix\n",
    "        \n",
    "        # Encode categorical features\n",
    "        for col in self.categorical_features:\n",
    "            self.label_encoders[col] = LabelEncoder()\n",
    "            df[col + '_encoded'] = self.label_encoders[col].fit_transform(df[col])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \"\"\"Transform test data using fitted preprocessor\"\"\"\n",
    "        # Create initial features\n",
    "        df = self.create_features(df)\n",
    "        \n",
    "        # Handle missing values using fitted imputers\n",
    "        numerical_features_matrix = self.numerical_imputer.transform(df[self.numerical_features])\n",
    "        df[self.numerical_features] = numerical_features_matrix\n",
    "        \n",
    "        # Now we can safely compute Age*Class\n",
    "        df['Age*Class'] = df['Age'] * df['Pclass']\n",
    "        \n",
    "        categorical_features_matrix = self.categorical_imputer.transform(df[self.categorical_features])\n",
    "        df[self.categorical_features] = categorical_features_matrix\n",
    "        \n",
    "        # Use fitted label encoders\n",
    "        for col in self.categorical_features:\n",
    "            df[col + '_encoded'] = self.label_encoders[col].transform(df[col])\n",
    "        \n",
    "        return df\n",
    "\n",
    "def train_models(X_train, y_train):\n",
    "    \"\"\"Train multiple models with cross-validation\"\"\"\n",
    "    models = {\n",
    "        'catboost': CatBoostClassifier(\n",
    "            iterations=1000,\n",
    "            learning_rate=0.02,\n",
    "            depth=8,\n",
    "            l2_leaf_reg=3,\n",
    "            random_seed=42,\n",
    "            verbose=False\n",
    "        ),\n",
    "        'random_forest': RandomForestClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=10,\n",
    "            random_state=42\n",
    "        ),\n",
    "        'gradient_boosting': GradientBoostingClassifier(\n",
    "            n_estimators=300,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=5,\n",
    "            random_state=42\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    trained_models = {}\n",
    "    for name, model in models.items():\n",
    "        # Perform cross-validation\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "        print(f\"{name} CV Score: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n",
    "        \n",
    "        # Train on full training data\n",
    "        model.fit(X_train, y_train)\n",
    "        trained_models[name] = model\n",
    "    \n",
    "    return trained_models\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    train_df = pd.read_csv('train.csv')\n",
    "    test_df = pd.read_csv('test.csv')\n",
    "    \n",
    "    # Initialize preprocessor\n",
    "    preprocessor = TitanicPreprocessor()\n",
    "    \n",
    "    # Preprocess training data\n",
    "    processed_train = preprocessor.fit_transform(train_df)\n",
    "    \n",
    "    # Prepare features for training\n",
    "    feature_cols = ([f\"{col}_encoded\" for col in preprocessor.categorical_features] + \n",
    "                   preprocessor.numerical_features + \n",
    "                   ['HasCabin', 'IsAlone', 'Fare_Per_Person', 'Age*Class'])\n",
    "    \n",
    "    X_train = processed_train[feature_cols]\n",
    "    y_train = processed_train['Survived']\n",
    "    \n",
    "    # Verify no missing values\n",
    "    assert not X_train.isnull().any().any(), \"Missing values found in training features\"\n",
    "    \n",
    "    # Train models\n",
    "    trained_models = train_models(X_train, y_train)\n",
    "    \n",
    "    # Process test data\n",
    "    processed_test = preprocessor.transform(test_df)\n",
    "    X_test = processed_test[feature_cols]\n",
    "    \n",
    "    # Verify no missing values in test data\n",
    "    assert not X_test.isnull().any().any(), \"Missing values found in test features\"\n",
    "    \n",
    "    # Make predictions\n",
    "    weights = {'catboost': 0.5, 'random_forest': 0.3, 'gradient_boosting': 0.2}\n",
    "    weighted_predictions = np.zeros(len(X_test))\n",
    "    \n",
    "    for name, model in trained_models.items():\n",
    "        weighted_predictions += model.predict_proba(X_test)[:, 1] * weights[name]\n",
    "    \n",
    "    final_predictions = (weighted_predictions > 0.5).astype(int)\n",
    "    \n",
    "    # Create submission file\n",
    "    submission = pd.DataFrame({\n",
    "        'PassengerId': test_df['PassengerId'],\n",
    "        'Survived': final_predictions\n",
    "    })\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    \n",
    "    # Print training metrics\n",
    "    print(\"\\nTraining Metrics:\")\n",
    "    for name, model in trained_models.items():\n",
    "        train_preds = model.predict(X_train)\n",
    "        print(f\"\\n{name} Performance:\")\n",
    "        print(classification_report(y_train, train_preds))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
